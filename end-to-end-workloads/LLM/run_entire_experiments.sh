#!/bin/bash

python run_various_configs.py --model_name_or_path /model_weights/meta-llama/Llama-3.1-8B-Instruct --use_kv_cache --ignore_eos --dtype bf16 --bf16 --use_hpu_graphs --use_flash_attention --flash_attention_recompute --flash_attention_causal_mask --reuse_cache --attn_softmax_bf16 --merged-log-name fix_input_len
python ../gaudi_spawn.py --use_deepspeed --world_size 4 run_various_configs.py --model_name_or_path /model_weights/meta-llama/Llama-3.1-70B-Instruct --use_kv_cache --ignore_eos --dtype bf16 --bf16 --use_hpu_graphs --use_flash_attention --flash_attention_recompute --flash_attention_causal_mask --reuse_cache --attn_softmax_bf16 --merged-log-name fix_input_len 
python ../gaudi_spawn.py --use_deepspeed --world_size 8 run_various_configs.py --model_name_or_path /model_weights/meta-llama/Llama-3.1-70B-Instruct --use_kv_cache --ignore_eos --dtype bf16 --bf16 --use_hpu_graphs --use_flash_attention --flash_attention_recompute --flash_attention_causal_mask --reuse_cache --attn_softmax_bf16 --merged-log-name fix_input_len 
python ../gaudi_spawn.py --use_deepspeed --world_size 2 run_various_configs.py --model_name_or_path /model_weights/meta-llama/Llama-3.1-70B-Instruct --use_kv_cache --ignore_eos --dtype bf16 --bf16 --use_hpu_graphs --use_flash_attention --flash_attention_recompute --flash_attention_causal_mask --reuse_cache --attn_softmax_bf16 --merged-log-name fix_input_len 
